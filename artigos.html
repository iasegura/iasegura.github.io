<!DOCTYPE html>
<html lang="pt">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Artigos - IA Segura</title>
    <link rel="stylesheet" href="css/estilos.css" />
    <link rel="stylesheet" href="css/artigos.css" />
  </head>
  <body>
    <header>
      <span class="hamburger-icon">&#9776;</span>
      <nav class="menu-hamburger">
        <ul>
          <li><a href="index.html">P√°gina Inicial</a></li>
          <li><a href="artigos.html">Saiba mais</a></li>
          <li><a href="time.html">Sobre</a></li>
          <li><a href="contato.html">Contato</a></li>
        </ul>
      </nav>
    </header>

    <main>
      <h1>Artigos sobre o tema</h1>
      <section class="articles-container">
        <div class="article">
          <h3>Deep Anomaly Detection with Outlier Exposure</h3>
          <br />

          <pre>
Dan Hendrycks
University of California, Berkeley
hendrycks@berkeley.edu
</pre
          >

          <pre>

Mantas Mazeika
University of Chicago
mantas@ttic.edu
</pre
          >

          <pre>

Thomas Dietterich
Oregon State University
tgd@oregonstate.edu
    </pre
          >

          <h4>ABSTRACT</h4>
          <p>
            √â importante detectar entradas an√¥malas ao implantar sistemas de
            aprendizado de m√°quina. O uso de entradas maiores e mais complexas
            no aprendizado profundo amplia a dificuldade de distinguir entre
            exemplos an√¥malos e dentro da distribui√ß√£o. Ao mesmo tempo, diversos
            dados de imagem e texto est√£o dispon√≠veis em enormes quantidades.
            Propomos aproveitar esses dados para melhorar a detec√ß√£o de
            anomalias profundas, treinando detectores de anomalias em um
            conjunto de dados auxiliar de outliers, uma abordagem que chamamos
            de Exposi√ß√£o a Outliers (OE). Isso permite que os detectores de
            anomalias generalizem e detectem anomalias n√£o vistas. Em
            experimentos abrangentes em tarefas de processamento de linguagem
            natural e vis√£o em pequena e grande escala, descobrimos que a
            Exposi√ß√£o a Outliers melhora significativamente o desempenho da
            detec√ß√£o. Tamb√©m observamos que modelos generativos de ponta
            treinados no CIFAR-10 podem atribuir probabilidades maiores √†s
            imagens SVHN do que √†s imagens CIFAR-10; usamos OE para mitigar esse
            problema. Al√©m disso, analisamos a flexibilidade e robustez da
            Exposi√ß√£o a Outliers e identificamos caracter√≠sticas do conjunto de
            dados auxiliar que melhoram o desempenho.
          </p>
          <a href="https://arxiv.org/pdf/1812.04606.pdf">PDF</a>
        </div>
        <div class="article">
          <h3>Locating and Editing Factual Associations in GPT</h3>
          <br />

          <p>Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov</p>

          <h4>ABSTRACT</h4>
          <p>
            Analisamos o armazenamento e a recupera√ß√£o de associa√ß√µes factuais
            em modelos de linguagem autoregressiva baseados em transformadores,
            encontrando evid√™ncias de que essas associa√ß√µes correspondem a
            c√°lculos localizados e diretamente edit√°veis. Primeiramente,
            desenvolvemos uma interven√ß√£o causal para identificar as ativa√ß√µes
            de neur√¥nios que s√£o decisivas nas previs√µes factuais de um modelo.
            Isso revela um conjunto distinto de etapas nos m√≥dulos de avan√ßo do
            feed de camadas intermedi√°rias que medeiam as previs√µes factuais
            durante o processamento de tokens de sujeito. Para testar nossa
            hip√≥tese de que esses c√°lculos correspondem √† recupera√ß√£o de
            associa√ß√µes factuais, modificamos os pesos de avan√ßo do feed para
            atualizar associa√ß√µes factuais espec√≠ficas usando a Edi√ß√£o de Modelo
            de Posto Um (ROME, na sigla em ingl√™s). Descobrimos que o ROME √©
            eficaz em uma tarefa padr√£o de edi√ß√£o de modelo de extra√ß√£o de
            rela√ß√£o zero-shot (zsRE), compar√°vel aos m√©todos existentes. Para
            realizar uma avalia√ß√£o mais sens√≠vel, tamb√©m avaliamos o ROME em um
            novo conjunto de dados de afirma√ß√µes contrafactuais, no qual ele
            simultaneamente mant√©m especificidade e generaliza√ß√£o, enquanto
            outros m√©todos sacrificam uma ou outra. Nossos resultados confirmam
            um papel importante para os m√≥dulos de avan√ßo do feed de camadas
            intermedi√°rias no armazenamento de associa√ß√µes factuais e sugerem
            que a manipula√ß√£o direta dos mecanismos computacionais pode ser uma
            abordagem vi√°vel para a edi√ß√£o de modelos. O c√≥digo, conjunto de
            dados, visualiza√ß√µes e um caderno de demonstra√ß√£o interativo est√£o
            dispon√≠veis <a href="https://rome.baulab.info/">nesta URL</a>
          </p>
          <a href="https://arxiv.org/pdf/2202.05262">PDF</a>
        </div>

        <div class="article">
          <h3>The Mythos of Model Interpretability</h3>
          <br />

          <p>Zachary C. Lipton</p>

          <h4>ABSTRACT</h4>
          <p>Modelos de aprendizado de m√°quina supervisionados apresentam not√°veis capacidades preditivas. Mas voc√™ pode confiar no seu modelo? Ele funcionar√° em implanta√ß√£o? O que mais ele pode dizer sobre o mundo? Queremos que os modelos sejam n√£o apenas bons, mas tamb√©m interpret√°veis. No entanto, a tarefa de interpreta√ß√£o parece subespecificada. Artigos fornecem diversas e, √†s vezes, motiva√ß√µes n√£o sobrepostas para a interpretabilidade e oferecem in√∫meras no√ß√µes de quais atributos tornam os modelos interpret√°veis. Apesar dessa ambiguidade, muitos artigos proclamam interpretabilidade axiom√°tica, sem mais explica√ß√µes. Neste artigo, buscamos refinar o discurso sobre a interpretabilidade. Primeiramente, examinamos as motiva√ß√µes subjacentes ao interesse em interpretabilidade, constatando que s√£o diversas e ocasionalmente discordantes. Em seguida, abordamos as propriedades e t√©cnicas dos modelos considerados interpret√°veis, identificando transpar√™ncia para humanos e explica√ß√µes p√≥s-hoc como no√ß√µes concorrentes. Ao longo do texto, discutimos a viabilidade e a desejabilidade das diferentes no√ß√µes e questionamos as afirma√ß√µes frequentemente feitas de que modelos lineares s√£o interpret√°veis e que redes neurais profundas n√£o s√£o.</p>
          <a href="https://arxiv.org/pdf/1606.03490">PDF</a>
        </div>


        <div class="article">
            <h3>Truthful AI: Developing and governing AI that does not lie</h3>
            <br />
  
            <p>Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, William Saunders</p>
  
            <h4>ABSTRACT</h4>
            <p>Em muitos contextos, mentir - o uso de falsidades verbais para enganar - √© prejudicial. Embora a mentira tenha sido tradicionalmente um assunto humano, sistemas de IA que fazem declara√ß√µes verbais sofisticadas est√£o se tornando cada vez mais comuns. Isso levanta a quest√£o de como devemos limitar o dano causado pelas "mentiras" da IA (ou seja, falsidades que s√£o selecionadas ativamente). A veracidade humana √© regida por normas sociais e leis (contra difama√ß√£o, perj√∫rio e fraude). As diferen√ßas entre a IA e os seres humanos apresentam a oportunidade de termos padr√µes de veracidade mais precisos para a IA e de elevar esses padr√µes ao longo do tempo. Isso poderia trazer benef√≠cios significativos para a epistemologia p√∫blica e a economia, e mitigar os riscos de cen√°rios futuros de IA piores.

                Estabelecer normas ou leis de veracidade para a IA exigir√° um trabalho significativo para: (1) identificar padr√µes claros de veracidade; (2) criar institui√ß√µes que possam julgar a ader√™ncia a esses padr√µes; e (3) desenvolver sistemas de IA que sejam robustamente verdadeiros.
                
                Nossas propostas iniciais para essas √°reas incluem: (1) um padr√£o para evitar "falsidades negligentes" (uma generaliza√ß√£o de mentiras que √© mais f√°cil de avaliar); (2) institui√ß√µes para avaliar sistemas de IA antes e ap√≥s a implanta√ß√£o no mundo real; e (3) treinamento expl√≠cito de sistemas de IA para serem verdadeiros por meio de conjuntos de dados selecionados e intera√ß√£o humana.
                
                Uma possibilidade preocupante √© que os mecanismos de avalia√ß√£o para os padr√µes de veracidade eventual possam ser capturados por interesses pol√≠ticos, levando √† censura prejudicial e √† propaganda. Evitar isso pode exigir aten√ß√£o cuidadosa. E, como a escala dos atos de fala da IA pode crescer dramaticamente nas pr√≥ximas d√©cadas, os padr√µes iniciais de veracidade podem ser particularmente importantes devido aos precedentes que estabelecem.</p>
            <a href="https://arxiv.org/pdf/2110.06674">PDF</a>
          </div>


      </section>
    </main>
    <footer>
      <p>üÑØ copyleft 2023 - IA segura</p>
    </footer>
  </body>
  <script type="text/javascript" src="js/menu.js"></script>
</html>
